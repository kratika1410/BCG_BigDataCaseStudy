# -*- coding: utf-8 -*-
"""Main

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jQEtNa_Hba1QqkbOnMKKYgNwyEvMVkEm
"""

!pip install pyspark

# importing all neccessary packages
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, when, count, countDistinct
from pyspark.sql.window import Window
from pyspark.sql import functions as F

import analytic  # Assuming that your analytics file is named 'analytics.py'

# main.py

def main():
    # Initialize Spark session
    spark = SparkSession.builder.appName("MySparkApp").getOrCreate()

    # Define input paths
    charges_path = "/Charges_use.csv"
    primary_person_path = "/Primary_Person_use.csv"
    units_path = "/Units_use.csv"
    damages_path = "/Damages_use.csv"
    restrict_path = "/Restrict_use.csv"

    # Load DataFrames from CSV files
    charges_df = spark.read.csv(charges_path, header=True, inferSchema=True)
    primary_person_df = spark.read.csv(primary_person_path, header=True, inferSchema=True)
    units_df = spark.read.csv(units_path, header=True, inferSchema=True)
    damages_df = spark.read.csv(damages_path, header=True, inferSchema=True)
    restrict_df = spark.read.csv(restrict_path, header=True, inferSchema=True)


    # Define output paths
    analysis1_output = "analysis1_result"
    analysis2_output = "analysis2_result"
    analysis3_output = "analysis3_result"
    analysis4_output = "analysis4_result"
    analysis5_output = "analysis5_result"
    analysis6_output = "analysis6_result"
    analysis7_output = "analysis7_result"
    analysis8_output = "analysis8_result"
    analysis9_output = "analysis9_result"
    analysis10_output = "analysis10_result"

    # Perform analyses
    analytic.analyze_crashes_with_male_fatalities(spark, charges_df, primary_person_df, analysis1_output)
    analytic.analyze_two_wheelers_booked(spark, units_df, analysis2_output)
    analytic.analyze_top_vehicle_makes_with_driver_death_no_airbag(spark, charges_df, primary_person_df, units_df, analysis3_output)
    analytic.analyze_hit_and_run_licensed_drivers(spark, primary_person_df, charges_df, analysis4_output)
    analytic.analyze_highest_accidents_without_females(spark, primary_person_df, analysis5_output)
    analytic.analyze_top_vehicle_makes_with_injuries(spark, units_df, analysis6_output)
    analytic.analyze_top_ethnicity_by_vehicle_style(spark, units_df, primary_person_df, analysis7_output)
    analytic.analyze_top_zip_codes_with_alcohol_contributions(spark, units_df, primary_person_df, analysis8_output)
    analytic.analyze_vehicles_with_high_damages_no_property_damage(spark, damages_df, units_df, analysis9_output)
    analytic.analyze_speeding_vehicles_with_top_colors_states(spark, charges_df, primary_person_df, units_df, analysis10_output)


    # Stop Spark session
    spark.stop()

    return analysis1_output, analysis2_output, analysis3_output, analysis4_output, analysis5_output, analysis6_output, analysis7_output, analysis8_output, analysis9_output, analysis10_output

if __name__ == "__main__":
    analysis1_output, analysis2_output, analysis3_output, analysis4_output, analysis5_output, analysis6_output, analysis7_output, analysis8_output, analysis9_output, analysis10_output = main()

    if analysis1_output and analysis2_output and analysis3_output and analysis4_output and analysis5_output and analysis6_output and analysis7_output and analysis8_output and analysis9_output and analysis10_output:
        # Initialize Spark session again for reloading results
        spark = SparkSession.builder.appName("MySparkApp").getOrCreate()

        # Find the number of crashes (accidents) in which number of males killed are greater than 2?
        result_analysis1 = spark.read.parquet(analysis1_output)
        print("Results of Analysis 1:")
        result_analysis1.show()

        # How many two wheelers are booked for crashes?
        result_analysis2 = spark.read.parquet(analysis2_output)
        print("Results of Analysis 2:")
        result_analysis2.show()

        # Determine the Top 5 Vehicle Makes of the cars present in the crashes in which driver died
        # and Airbags did not deploy.
        result_analysis3 = spark.read.parquet(analysis3_output)
        print("Results of Analysis 3:")
        result_analysis3.show()

        # Determine number of Vehicles with driver having valid licences involved in hit and run?
        result_analysis4 = spark.read.parquet(analysis4_output)
        print("Results of Analysis 4:")
        result_analysis4.show()

        # Which state has highest number of accidents in which females are not involved?
        result_analysis5 = spark.read.parquet(analysis5_output)
        print("Results of Analysis 5:")
        result_analysis5.show()

        # Which are the Top 3rd to 5th VEH_MAKE_IDs that contribute to a largest number of injuries including death
        result_analysis6 = spark.read.parquet(analysis6_output)
        print("Results of Analysis 6:")
        result_analysis6.show()

        # For all the body styles involved in crashes, mention the top ethnic user group of each unique body style
        result_analysis7 = spark.read.parquet(analysis7_output)
        print("Results of Analysis 7:")
        result_analysis7.show()

        # Among the crashed cars, what are the Top 5 Zip Codes with highest number crashes with alcohols
        # as the contributing factor to a crash (Use Driver Zip Code)
        result_analysis8 = spark.read.parquet(analysis8_output)
        print("Results of Analysis 8:")
        result_analysis8.show()

        # Count of Distinct Crash IDs where No Damaged Property was observed and
        # Damage Level (VEH_DMAG_SCL~) is above 4 and car avails Insurance
        result_analysis9 = spark.read.parquet(analysis9_output)
        print("Results of Analysis 9:")
        result_analysis9.show()

        # Determine the Top 5 Vehicle Makes where drivers are charged with speeding related offences,
        # has licensed Drivers, used top 10 used vehicle colours and has car licensed with the Top 25 states
        # with highest number of offences (to be deduced from the data)
        result_analysis10 = spark.read.parquet(analysis10_output)
        print("Results of Analysis 10:")
        result_analysis10.show()

        # Stop Spark session
        spark.stop()
    else:
        print("Output path not defined.")